---
title: "Takehome Midterm"
author: "Rob Han"
date: "04 Mar 2020"
output:
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

For the takehome midterm you will have 48 hours to complete the objectives listed below. The deadline for submission is 18 March 2020 at 2pm EST.

Date/Time started: March 15, 2020 11:30 AM
Date/Time completed: March 16, 2020 10:45 PM

You will be graded on the following criteria:

* Completion of the objectives
* Successful knitting of the pdf
* Readability (tidyness) of Rmd code
* Acknowledgement of resources

## Loading Libraries

Load all of your libraries in this code block. Indicate why each library is necessary.

```{r Load Libraries, include=FALSE}
if (!require("MASS")) install.packages("MASS"); library(MASS)
# MASS is needed for the negative binomial glm function glm.nb()
if (!require("gridExtra")) install.packages("gridExtra"); library(gridExtra)
if (!require("cowplot")) install.packages("cowplot"); library(cowplot)
if (!require("UsingR")) install.packages("UsingR"); library(UsingR)
if (!require("e1071")) install.packages("e1071"); library(e1071)  
if (!require("devtools")) install.packages("devtools"); library(devtools)
install_github("kassambara/ggcorrplot")
library(ggcorrplot)
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)
# tidyverse is needed for ggplot2, readr, and dplyr
# Always load tidyverse last so that its functions will not get masked by other packages
# It is also good practice to specify dplyr::filter() and dplyr::select() functions
```

## Objectives for Midterm Exam

* [ ] Import, clean, merge data tables
* [ ] Present graphical summary of Dengue incidence data
* [ ] Data exploration of potential explanatory variables
* [ ] Test Benchmark model of Dengue incidence
* [ ] Improve model of Dengue incidence

## Background

This dataset should be familiar from Lab 6. We will be using the Dengue dataset from a Driven Data competition: 
https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/

The data for this competition comes from multiple sources aimed at supporting the Predict the Next Pandemic Initiative (https://www.whitehouse.gov/blog/2015/06/05/back-future-using-historical-dengue-data-predict-next-epidemic). 
Dengue surveillance data is provided by the U.S. Centers for Disease Control and prevention, as well as the Department of Defense's Naval Medical Research Unit 6 and the Armed Forces Health Surveillance Center, in collaboration with the Peruvian government and U.S. universities. 
Environmental and climate data is provided by the National Oceanic and Atmospheric Administration (NOAA), an agency of the U.S. Department of Commerce.

The data is provided in two separate files:

1. dengue_features_train: weekly weather and vegetation data for two cities
2. dengue_labels_train: weekly number of dengue cases in each city

There are two cities, San Juan, Puerto Rico and Iquitos, Peru, with test data for each city spanning 5 and 3 years respectively. The data for each city have been concatenated along with a city column indicating the source: *sj* for San Juan and *iq* for Iquitos. 

```{r Read Data}
dengue_features_train <- read_csv("https://s3.amazonaws.com/drivendata/data/44/public/dengue_features_train.csv")
dengue_labels_train <- read_csv("https://s3.amazonaws.com/drivendata/data/44/public/dengue_labels_train.csv")
```

## Feature Descriptions

You are provided the following set of information on a (year, weekofyear) timescale:

(Where appropriate, units are provided as a _unit suffix on the feature name.)

City and date indicators

- city – City abbreviations: sj for San Juan and iq for Iquitos
- week_start_date – Date given in yyyy-mm-dd format

NOAA's GHCN daily climate data weather station measurements

- station_max_temp_c – Maximum temperature
- station_min_temp_c – Minimum temperature
- station_avg_temp_c – Average temperature
- station_precip_mm – Total precipitation
- station_diur_temp_rng_c – Diurnal temperature range

PERSIANN satellite precipitation measurements (0.25x0.25 degree scale)

- precipitation_amt_mm – Total precipitation

NOAA's NCEP Climate Forecast System Reanalysis measurements (0.5x0.5 degree scale)

- reanalysis_sat_precip_amt_mm – Total precipitation
- reanalysis_dew_point_temp_k – Mean dew point temperature
- reanalysis_air_temp_k – Mean air temperature
- reanalysis_relative_humidity_percent – Mean relative humidity
- reanalysis_specific_humidity_g_per_kg – Mean specific humidity
- reanalysis_precip_amt_kg_per_m2 – Total precipitation
- reanalysis_max_air_temp_k – Maximum air temperature
- reanalysis_min_air_temp_k – Minimum air temperature
- reanalysis_avg_temp_k – Average air temperature
- reanalysis_tdtr_k – Diurnal temperature range

Satellite vegetation - Normalized difference vegetation index (NDVI) - NOAA's CDR Normalized Difference Vegetation Index (0.5x0.5 degree scale) measurements

- ndvi_se – Pixel southeast of city centroid
- ndvi_sw – Pixel southwest of city centroid
- ndvi_ne – Pixel northeast of city centroid
- ndvi_nw – Pixel northwest of city centroid

# Takehome Exam Begins here

As a reminder, you may consult your previous homework and group projects, textbook and other readings, and online resources.
Online resources may be used to research ways to solve each problem, but you may not pose questions in online forums about the specific assignment.
You may consult with Prof. Field or with other classmates about technical problems (e.g. where to find a file), but not about how to answer any of the questions.

## (1) Data Wrangling

Use this section to manipulate the two data frames.
1. Follow the Exploratory Data Analysis Checklist (below) to verify the imported data
    a. Check that each variable is the appropriate data class and has values that makes sense
    b. For external verification, at a minimum, check that the annual Dengue incidence numbers for each city are realistic
2. Merge the two data frames, verifying that no information was lost during the merge
3. Check the data for NAs both before and after the merge (note that eliminating all rows or columns with NAs will have consequences)

Import for this data frame looks good, everything seems to have come in intact with appropriate items in each row/column. We count 548 NA's 
```{r Checking features}
str(dengue_features_train)
cat("rows", nrow(dengue_features_train))
cat("\ncolumns", ncol(dengue_features_train))
cat("\nNA's", sum(is.na(dengue_features_train)))
head(dengue_features_train)
tail(dengue_features_train)
```

Good import here as well, no NA's found.
```{r Checking labels}
str(dengue_labels_train)
cat("rows", nrow(dengue_labels_train))
cat("\ncolumns", ncol(dengue_labels_train))
cat("\nNA's", sum(is.na(dengue_labels_train)))
head(dengue_labels_train)
tail(dengue_labels_train)
```

### Validate with at least one external data source
We have a publication from the CDC stating San Juan reported had 4,677 diagnosed cases of dengue in 1998 our dataset
when we filter for San Juan in 1998 says we have 4,595 cases which appears close enough to what was reported by the
CDC.

https://www.cdc.gov/mmwr/preview/mmwrhtml/00055624.htm
```{r Validating}
sanjuan1994 <- dplyr::filter(dengue_labels_train, year == "1998"&city == "sj")
sum(sanjuan1994$total_cases)
```

### Merging the features and labels data frames

Although there are dplyr functions for data frame merging, the base `merge()` function is easier to use.

Taking a cursory look at the two data frames, it looks like the only column that needs to be merged is total_cases
in dengue_labels_train. 
```{r Merge datasets}
dengue_features_train$total_cases <- dengue_labels_train$total_cases
head(dengue_features_train)
```

Checking the data frame post merge we see everything looks as we'd expect.
```{r Checking features again}
str(dengue_features_train)
cat("rows", nrow(dengue_features_train))
cat("\ncolumns", ncol(dengue_features_train))
cat("\nNA's", sum(is.na(dengue_features_train)))
head(dengue_features_train)
tail(dengue_features_train)
```

### Dealing with the NAs

Check out the `tidyr::fill()` function for one way to take care of NAs.

Earlier we mentioned thatnthere are 548 NA's in the data frame. One way for dealing with these is the na.omit function
which will remove rows from the dataset where NA's are present. After running the function, we end up with
approximately 200 less rows than we started with.
```{r deal with NA}
dengue_features_train <- na.omit(dengue_features_train)
head(dengue_features_train)
nrow(dengue_features_train)
```


## (2) What is the average number of cases of Dengue for each week of the year for each city?

Provide a publication-quality graphic to present this comparison. 
The graph should span a single year, with the average incidence for each week of the year. 
You are encouraged to explore options, but only your final graph in this section will be used to evaluate this objective.
Consider the most effective way to illustrate any trends or important comparisons within the data.

In order to get the averages for each week we ought to filter out unnecessary data such as weather, then create a new
dataset as the data frame with the averages will be a different size than the original one. Alternatively we could have
used the dengue_labels_train dataset which has only the information we need for this visualization and no NA's.
We can create the new dataframe with the calculated averages using the aggregate() function.

https://stackoverflow.com/questions/23775683/calculate-average-value-over-multiple-years-for-each-hour-and-day
```{r}
dengue_simple <- dplyr::select(dengue_features_train, city, weekofyear, total_cases, everything())
dengue_simple <- dplyr::select(dengue_simple, 1:3)
head(dengue_simple)
dengue_avg_week <- aggregate(dengue_simple$total_cases, by = list(dengue_simple$city, dengue_simple$weekofyear), mean)
dengue_avg_week <- rename(dengue_avg_week, city = Group.1, weekofyear = Group.2, avg_cases = x)
head(dengue_avg_week)
```
I thought a marked scatterplot would be best for this visualization as it will show individual datapoints and give a 
better sense of possible trends than either a scatterplot or line plot alone. Checking out the visualization we see
that cases of dengue increas towards the third quarter of the year and San Juan has higher dengue incidence than
Iquitos throughout the entire year especially between weeks 25 and 50

Note that 53 comes up as a week in the plot (line extends slightly past 52). I'm not sure why but at some points in the
dataset a 53rd week was recorded. It's clear that some 52nd weeks may have been mislabeled. It's not a big
deal and can be fixed.

https://dplyr.tidyverse.org/reference/select.html
```{r}
ggplot(dengue_avg_week, aes(x = weekofyear, y = avg_cases, color = city)) +
  geom_point() + 
  geom_line() +
  scale_x_continuous(breaks = round(seq(min(dengue_avg_week$weekofyear), max(dengue_avg_week$weekofyear), by = 3))) +
  xlab("Week") +
  ylab("Average number of dengue cases") +
  theme_cowplot()
```

For the sake of not editing the original data we're going to create a new dataset with the fix. I suspect that for a 
couple of the years week data was misinputted as 53 when it's supposed to be 52 since that's how it is in the csv file.
With that in mind we will try to change values from 53 to 52. Since I'm not super sure about what's going on we will
only use this modified dataset to revisualize the last figure in case there's a large change. in order to do this
we'll use the replace() function to replace instances of 53 with 52.

Alternatively if we didn't have a suspicion about the error we could have filtered out week 53 with the dplyr::filter()
function.

https://stackoverflow.com/questions/5824173/replace-a-value-in-a-data-frame-based-on-a-conditional-if-statement
```{r removing 53rd week}
no_53 <- dengue_simple
no_53$weekofyear <- replace(no_53$weekofyear, no_53$weekofyear == 53,  52)

no_53_df <- aggregate(no_53$total_cases, by = list(no_53$city, no_53$weekofyear), mean)
no_53_df <- rename(no_53_df, city = Group.1, weekofyear = Group.2, avg_cases = x)
head(no_53_df)

ggplot(no_53_df, aes(x = weekofyear, y = avg_cases, color = city)) +
  geom_line() + 
  geom_point() +
  scale_x_continuous(breaks = round(seq(min(no_53$weekofyear), max(no_53$weekofyear), by = 3))) +
  xlab("Week") +
  ylab("Average number of dengue cases") +
  theme_cowplot()
```

In hindsight, the shape of the data doesn't look too different and the 53rd week might actually represent a 53rd week
since 7 does not divide evenly into 365. Regardless, the replace() function is a really neat tool since otherwise I'd
have had to create a for loop with logic to get the same result.

## (3) Data exploration of potential explanatory variables

Consider whether transforming any of the variables might increase the statistical power available.
Explore the correlation of the potential explanatory variables with each other and with dengue incidence.
Present a two or more publication-quality graphics to illustrate your most important findings.

### Exploration

Lets take a look at a few variables in the context of their contribution to dengue incidence. We know that dengue is a 
mosquito borne illness and mosquitos typically come out when the weather is warm so that could be a good starting
point.

First we'll check the distribtion of the data. From the Shapiro test, it looks like the data is not normally
distributed (avg_t: p = 3.487e-16, min_t: p = 1.143e-10, max_t: p = 1.801e-11). All three variables have a left skew
(avg_t: -0.622526, min_t: -0.2843803, max_t: -0.2714032). According to some sources a skewness between -0.8 and 0.8 
is acceptable so we'll say the distribution is approximately symmetric.

The log10 transformation did not appear to greatly improve the distribution so we will forego that.

https://www.researchgate.net/post/What_is_the_acceptable_range_of_skewness_and_kurtosis_for_normal_distribution_of_data
```{r Distribution check}
simple.eda((dengue_features_train$station_avg_temp_c))
simple.eda(dengue_features_train$station_min_temp_c)
simple.eda(dengue_features_train$station_max_temp_c)

shapiro.test((dengue_features_train$station_avg_temp_c))
skewness(dengue_features_train$station_avg_temp_c)
shapiro.test((dengue_features_train$station_min_temp_c))
skewness(dengue_features_train$station_min_temp_c)
shapiro.test((dengue_features_train$station_max_temp_c))
skewness(dengue_features_train$station_max_temp_c)
```

We'll do something similar to earlier but apply the aggregate() function to every variable using city and week of year
as grouping variables. First we'll remove year and week start date though because those won't give us meaningful 
information.
```{r Aggregate everything}
dengue_features_train <- dplyr::select(dengue_features_train, -(year), -(week_start_date))
avg_all <- aggregate(.~city + weekofyear, dengue_features_train, mean)
head(avg_all)
```

Our visualization is not great, we can see some hints at a trend where increases in temperature increase the number of
dengue cases. The shaded regions are from a nonbinomial glm using the MASS package.  
We see a lot of data points fall outside of the shading so maybe this visualization isn't as informative as we hoped.

https://ggplot2.tidyverse.org/reference/geom_smooth.html
https://www.dummies.com/programming/r/how-to-round-off-numbers-in-r/
https://stats.stackexchange.com/questions/164889/how-to-deal-with-non-integer-warning-from-negative-binomial-glm

Perhaps other variables will present better results.
```{r Plotting temperature data with dengue case data}
data = avg_all
x = data$reanalysis_avg_temp_k
y = trunc(data$total_cases) # Truncate because glm.nb is expecting (whole number) counts here

ggplot(avg_all, aes(x =  x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() + 
  scale_x_continuous(breaks = round(seq(min(avg_all$station_min_temp_c), max(avg_all$station_max_temp_c), by = 3))) +
  xlab("Temperature (C)") +
  ylab("Average number of dengue cases") +
  theme_cowplot()
```

Now looking at humidity we see that San Juan has lower average humidity than Iquitos. This lower humidity may be 
associated with higher incidence of dengue but we can't be sure yet. We still see a lot of variability here. This is 
also a pretty strange looking graph so maybe the other measure of humidity will give something better.
```{r checking humidity}
data = avg_all
x = data$reanalysis_relative_humidity_percent
y = trunc(data$total_cases)

ggplot(data, aes(x = x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() +
  xlab("Humidity (%)") +
  ylab("Average dengue cases") +
  theme_cowplot()
```

This visualization of humidity appears to show a stronger relationship between humidity and dengue incidence as an 
positive trend is clearly evident.
```{r checking humidity again}
data = avg_all
x = data$reanalysis_specific_humidity_g_per_kg 
y = trunc(data$total_cases)

ggplot(data, aes(x = x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() +
  xlab("Average humidity (g/kg)") +
  ylab("Average dengue cases") +
  theme_cowplot()
```

It's kinda interesting that when we look at humidity here we get an eerily similar visualization as we do when looking
at the second humidity figure. Are the two related?

```{r checking dewpoint temp}
data = avg_all
x = data$reanalysis_dew_point_temp_k
y = trunc(data$total_cases)  
ggplot(data, aes(x = x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() +
  xlab("Average dewpoint temperature (K)") +
  ylab("Average dengue cases") +
  theme_cowplot()
```

Unsurprisingly they are at least somewhat related. We see that increases in humidity are correlated with increases in
dewpoint temperature. 
```{r Comparing humidity and dewpoint temp, warning = FALSE}
data = avg_all
x = data$reanalysis_dew_point_temp_k
y = trunc(data$reanalysis_specific_humidity_g_per_kg)
ggplot(data, aes(x = x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() +
  xlab("Average dewpoint temperature (K)") +
  ylab("Average humidity (g/kg)") +
  theme_cowplot()
```

Looking at precipitation isn't very informative, we barely capture any of the datapoints from iq.
```{r checking precipitation}
data = avg_all
x = data$reanalysis_precip_amt_kg_per_m2
y = trunc(data$total_cases)  
ggplot(data, aes(x = x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() +
  xlab("Precipitation (kg/m2)") +
  ylab("Average dengue cases") +
  theme_cowplot()
```

All in all we see that San Juan has more cases of dengue than Izquitos. There are some subtle relationships between a
few of the meteorological variables such as temperature and wetness and the number of dengue cases but they are as I 
said subtle.

Explore the data here.

### Presentation

Most important findings repeated from above.

```{r Plotting temperature data with dengue case data 2}
data = avg_all
x = data$reanalysis_avg_temp_k
y = trunc(data$total_cases) # Truncate because glm.nb is expecting (whole number) counts here

ggplot(avg_all, aes(x =  x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() + 
  scale_x_continuous(breaks = round(seq(min(avg_all$station_min_temp_c), max(avg_all$station_max_temp_c), by = 3))) +
  xlab("Temperature (C)") +
  ylab("Average number of dengue cases") +
  theme_cowplot()
```

```{r checking humidity again 2}
data = avg_all
x = data$reanalysis_specific_humidity_g_per_kg 
y = trunc(data$total_cases)

ggplot(data, aes(x = x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() +
  xlab("Average humidity (g/kg)") +
  ylab("Average dengue cases") +
  theme_cowplot()
```

## (4) Dengue incidence model

Use a generalized linear model to determine the best model for the weekly incidence of Dengue.
At a first pass consider the "Benchmark" model described here: https://shaulab.github.io/DrivenData/DengAI/Benchmark.html
This model is calculated separately for San Jose and Iquitos and only uses the following variables:
 - reanalysis_specific_humidity_g_per_kg
 - reanalysis_dew_point_temp_k 
 - station_avg_temp_c
 - station_min_temp_c
 
 
The code for the Benchmark model uses a machine learning approach to optimize the model.
You should use the model selection approach that we have used in BIOL 364, instead.
The total_cases outcome variable is a count - statistically it is a binomial variable that has been summed up over a period of time (a week, in this case). 
Generalized linear models should use a negative binomial distribution (as opposed to a Gaussian distribution, which is what `glm()` assumes) for this type of data.
To fit a negative binomial distribution use `glm.nb()` from the package `MASS` instead of the `glm()` function from `stats`.

Like https://shaulab.github.io/DrivenData/DengAI/Benchmark.html described in their correlation plot, we see that 
total_cases does not have much correlation with the other variables. However, when we apply the correlation plot to
the averaged dataset we find strongish correlations to minimum air temperatures.
```{r}
dengue_features_train %>%
  dplyr::select(precipitation_amt_mm: total_cases) %>%
  as.matrix() -> dengue_matrix

dcorr <- round(cor(dengue_matrix, use = "complete.obs"), 1)
print(dcorr)
# Compute a matrix of correlation p-values
p.mat <- cor_pmat(log10(dengue_matrix))
print(p.mat)
# Visualize the correlation matrix
ggcorrplot(dcorr, hc.order = TRUE, type = "lower",
     outline.col = "white", lab = TRUE)

avg_all %>%
  dplyr::select(precipitation_amt_mm: total_cases) %>%
  as.matrix() -> dengue_avg_matrix

dcorr_avg <- round(cor(dengue_avg_matrix, use = "complete.obs"), 1)
print(dcorr_avg)
# Compute a matrix of correlation p-values
p_avg.mat <- cor_pmat(log10(dengue_avg_matrix))
print(p_avg.mat)
# Visualize the correlation matrix
ggcorrplot(dcorr_avg, hc.order = TRUE, type = "lower",
     outline.col = "white", lab = TRUE)
```
 
I thought it would be nice to reproduce the other correlation figure from the link but its not great so we'll ignore
it.
```{r total_cases correlations}
sort(dcorr[17,-17]) %>%  
  as.data.frame %>% 
  `names<-`('correlation') %>%
ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) + 
  geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits = c(-.15,.25)) +
  labs(title = 'Correlations', x = NULL, y = NULL) + coord_flip() -> cor1

sort(dcorr_avg[17,-17]) %>%  
  as.data.frame %>% 
  `names<-`('correlation') %>%
ggplot(aes(x = reorder(row.names(.), -correlation), y = correlation, fill = correlation)) + 
  geom_bar(stat='identity', colour = 'black') + scale_fill_continuous(guide = FALSE) + scale_y_continuous(limits = c(-.15,.25)) +
  labs(title = 'Avgs Correlations', x = NULL, y = NULL) + coord_flip() -> cor2

grid.arrange(cor1, cor2, nrow = 1)
```


Lets do a cursory glm of some suspected variables as predictors. Earlier we posed temperature as a possible predictor 
so we'll run one with that. We'll also look at minimum air temperature and specific humidity which we identified from 
the correlation plots. We'll do the glm calculations using glm.nb() from MASS because the data is nonbinomial.
```{r}
avg_t_glm <- glm.nb(total_cases ~ (reanalysis_air_temp_k), data = dengue_features_train)
avg_t_glm2 <- glm.nb(trunc(total_cases) ~ (reanalysis_air_temp_k), data = avg_all)

min_t_glm <- glm.nb(total_cases ~ (reanalysis_min_air_temp_k), data = dengue_features_train)
min_t_glm2 <- glm.nb(trunc(total_cases) ~ (reanalysis_min_air_temp_k), data = avg_all)

hum_glm <- glm.nb(total_cases ~ (reanalysis_specific_humidity_g_per_kg), data = dengue_features_train)
hum_glm2 <- glm.nb(trunc(total_cases) ~ (reanalysis_specific_humidity_g_per_kg), data = avg_all)
```

Looking at our results we see that the avg min temp model is the most parsimonious fit from both the original merged
dengue dataset and the averaged dataset.
```{r}
cat("\ntemp:", avg_t_glm$aic)
cat("\nmin temp:", min_t_glm$aic)
cat("\nspecific humidity:", hum_glm$aic)

cat("\navg temp:", avg_t_glm2$aic)
cat("\navg min temp:", min_t_glm2$aic)
cat("\navg specific humidity:", hum_glm2$aic)
```

When we actual take a look at the model for this we get some pretty interesting graphs. With the averaged data we see a
pretty clear trend where higher avg min temperature is associated with greater average dengue incidence.
```{r nb.glm model for avg min temp}
data = dengue_features_train
x = data$reanalysis_min_air_temp_k
y = trunc(data$total_cases)

ggplot(data, aes(x = x, y = y, color = city)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = MASS::glm.nb) + 
  xlab("Min temperature (K)") +
  ylab("Dengue cases") +
  theme_cowplot()

data = avg_all
x = data$reanalysis_min_air_temp_k
y = trunc(data$total_cases)

ggplot(data, aes(x = x, y = y, color = city)) +
  geom_smooth(method = MASS::glm.nb) + 
  geom_point() +
  xlab("Average min temperature (K)") +
  ylab("Average dengue cases") +
  theme_cowplot()
```

## (5) Extend the Benchmark model

Consider and test the inclusion of additional explanatory variables to improve the Benchmark model. 

The benchmark model uses only a few of all the variables so we'll use dropterm() to look at all the variables and see
if we can find a preferred model that the benchmark model did not look at. 

Here we do get several variables which reach the significance threshold (ndvi_se: Pr = 0.026456,
reanalysis_dew_point_temp_k: Pr = 0.000193, reanalysis_specific_humidity_g_per_kg: Pr = 1.60e-08, station_avg_temp_c: 
Pr = 0.013404, station_diur_temp_rng_c: Pr = 0.024334, station_min_temp_c: Pr = 0.028191)

https://stackoverflow.com/questions/22580379/how-do-i-exclude-specific-variables-from-a-glm-in-r
```{r Dropterms}
fullglm = MASS::glm.nb(total_cases ~. , data = dengue_features_train)
summary(fullglm)
```

Let's drop some terms
```{r}
dropterm(fullglm)
```

We'll drop reanalysis_max_air_temp_k for starters.
```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k, data = dengue_features_train)
dropterm(fullglm)
```
Strangely min temp is being suggested as the dropterm.
```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent -station_max_temp_c, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent -station_max_temp_c -station_precip_mm, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent -station_max_temp_c -station_precip_mm -ndvi_ne, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent -station_max_temp_c -station_precip_mm -ndvi_ne -reanalysis_tdtr_k, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent -station_max_temp_c -station_precip_mm -ndvi_ne -reanalysis_tdtr_k -reanalysis_precip_amt_kg_per_m2, data = dengue_features_train)
dropterm(fullglm)
```
Uhh we'll drop the terms that don't change the AIC now(?)
```{r}
fullglm = MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent -station_max_temp_c -station_precip_mm -ndvi_ne -reanalysis_tdtr_k -reanalysis_precip_amt_kg_per_m2 -precipitation_amt_mm, data = dengue_features_train)
dropterm(fullglm)
```

```{r}
fullglm <- MASS::glm.nb(total_cases ~. -reanalysis_max_air_temp_k -ndvi_sw -reanalysis_min_air_temp_k -reanalysis_relative_humidity_percent -station_max_temp_c -station_precip_mm -ndvi_ne -reanalysis_tdtr_k -reanalysis_precip_amt_kg_per_m2 -precipitation_amt_mm -reanalysis_sat_precip_amt_mm, data = dengue_features_train)
dropterm(fullglm)
```

Ok now we really can't drop any more terms. There are no longer any terms that when dropped will reduce the AIC of the
model.

```{r}
summary(fullglm)

benchmark <- MASS::glm.nb(total_cases ~ reanalysis_specific_humidity_g_per_kg + reanalysis_dew_point_temp_k + station_avg_temp_c + station_min_temp_c, data = dengue_features_train)
# Compare
cat("dropterm result: ", fullglm$aic)
cat("\nbenchmark result: ", benchmark$aic)
```

When we compare our two models, we found 11 "good" variables and had a lower AIC (9147.673) than the benchmark
(9421.378). It should be noted that the benchmark seperated the two cities so that would not be incuded in their model.
Had we dropped city our AIC would have risen to 9186.3 which is still lower than the benchmark model. 

# Acknowledgements

Fallon tried to help me with binning data but golly I just don't get it.

Online resources are refrenced as they appear in the code.